% Set up the document
\documentclass{article}

% Page size
\usepackage[
    letterpaper,]{geometry}
\usepackage{changepage}

% Lines between paragraphs
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0pt}

% Math
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{commath}

% Number sets
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

% Links
\usepackage{hyperref}

% Page numbers at top right
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{AMATH 751 assignment 1} \\
\textbf{Matt Wiens} \\
\textbf{2020-08-28}

1. Show that if $\phi(t)$ is a solution of the IVP
%
\begin{equation}
    x^\prime = f(x), \quad x(t_0) = x_0
    \label{eq:1-1}
\end{equation}
%
defined on $\R$, then $\phi(t + t_0)$ is a solution of the IVP
%
\begin{equation}
    x^\prime = f(x), \quad x(0) = x_0
    \label{eq:1-2}
\end{equation}
%
on $\R$. Is it still true if $f(x)$ is replaced by $f(t, x)$?

\textit{Solution.}
Let $\phi$ be a solution of~\eqref{eq:1-1}. Define
$\tilde{\phi}(t) \coloneqq \phi(t + t_0)$. Then
%
\begin{equation}
    \tilde{\phi}(0) = \phi(0 + t_0) = \phi(t_0) = x_0
    \label{eq:1-3}
\end{equation}
%
and
%
\begin{equation}
    \tilde{\phi}^\prime(t) = \phi^\prime(t + t_0) = f(\phi(t + t_0)) = f(\tilde{\phi}(t))
    \label{eq:1-4}
\end{equation}
%
(the first equality comes from a trivial application of the chain rule). Hence
we can conclude that $\tilde{\phi}$ satisfies~\eqref{eq:1-2}.

However, if $f$ is dependent on $t$, then~\eqref{eq:1-3} would still hold but
for~\eqref{eq:1-4} we would have, in general,
%
\begin{equation*}
    \tilde{\phi}^\prime(t)
        = \phi^\prime(t + t_0)
        = f(t + t_0, \phi(t + t_0))
        = f(t + t_0, \tilde{\phi}(t))
        \neq f(t, \tilde{\phi}(t))
        ,
\end{equation*}
%
so $\tilde{\phi}$ would no longer solve the IVP.

\newpage

2. Show that each of the functions
%
\begin{align*}
    |x|_1 &= \sum_{i = 1}^n |x_i|, \\
    |x|_2 &= \sqrt{\sum_{i = 1}^n x_i^2}, \\
    |x|_\infty &= \max_{1 \leq i \leq n} |x_i|
\end{align*}
%
defines a norm on $\R^n$.

\textit{Solution.}
The first property that norms $N$ must satisfy is that $N(x) \geq 0$ and
$N(x) = 0$ if and only if $x = 0$. Quite trivially, we can see
that $|\cdot|_1$, $|\cdot|_2$, and $|\cdot|_\infty$ are all non-negative
and that $|0|_1 = |0|_2 = |0|_\infty = 0$. Now suppose $x \neq 0$ where $x \in \R^n$,
and let $j$ be such that $x_j \neq 0$. Then we have
%
\begin{align*}
    |x|_1 &= \sum_{i = 1}^n |x_i| \geq |x_j| > 0, \\
    |x|_2 &= \sqrt{\sum_{i = 1}^n x_i^2} \geq \sqrt{x_j^2} > 0, \\
    |x|_\infty &= \max_{1 \leq i \leq n} |x_i| \geq |x_j| > 0.
\end{align*}
%
Hence for all three functions $|\cdot|$, we have that $|x| = 0$ if and only if $x = 0$.
Therefore all three functions satisfy the first property in the definition of a norm.

The second property that norms must satisfy is that $N(x + y) \leq N(x) + N(y)$.
Let $x, y \in \R^n$, then we have that, using the triangle inequality,
%
\begin{align*}
    |x + y|_1
        &= \sum_{i = 1}^n |x_i + y_i| \\
        &\leq \sum_{i = 1}^n (|x_i| + |y_i|) \\
        &= \sum_{i = 1}^n |x_i| + \sum_{i = 1}^n |y_i| \\
        &= |x|_1 + |y|_1
        .
\end{align*}
%
We also have that, using the Cauchy-Schwarz inequality,
%
\begin{align*}
    |x + y|_2^2
        &= \sum_{i = 1}^n (x_i + y_i)^2 \\
        &= \sum_{i = 1}^n (x_i^2 + 2 x_i y_i +  y_i^2) \\
        &= \sum_{i = 1}^n x_i^2
            + \sum_{i = 1}^n  y_i^2
            + 2 \sum_{i = 1}^n x_i y_i \\
        &= |x|_2^2 + |y|_2^2 + 2 \langle x, y \rangle \\
        &\leq |x|_2^2 + |y|_2^2 + 2 |x|_2 |y|_2 \\
        &= (|x|_2 + |y|_2)^2
        .
\end{align*}
%
Taking square roots (and remembering that we showed that $|\cdot|_2$
is non-negative above), we obtain
%
\begin{equation*}
    |x + y|_2 \leq |x|_2 + |y|_2
    .
\end{equation*}
%
Additionally, have that
%
\begin{align*}
    |x + y|_\infty
        &= \max_{1 \leq i \leq n} |x_i + y_i| \\
        &\leq \max_{1 \leq i \leq n} |x_i|
            + \max_{1 \leq i \leq n} |y_i| \\
        &= |x|_\infty + |y|_\infty
        .
\end{align*}
%
Hence all three functions satisfy the second property in the definition of a norm.

Finally, the third property that norms must satisfy is that for all $\alpha \in \R$,
$N(\alpha x) = |\alpha| N(x)$. Let $x \in \R^n$ and $\alpha \in \R$. Then we have that
%
\begin{align*}
    |\alpha x|_1
        &= \sum_{i = 1}^n |\alpha x_i|
        = \sum_{i = 1}^n |\alpha| |x_i|
        = |\alpha| \sum_{i = 1}^n |x_i|
        = |\alpha| |x|_1
        , \\
    |\alpha x|_2
        &= \sqrt{\sum_{i = 1}^n (\alpha x_i)^2}
        = \sqrt{\sum_{i = 1}^n \alpha^2 x_i^2}
        = \sqrt{\alpha^2 \sum_{i = 1}^n x_i^2}
        = \sqrt{\alpha^2} \sqrt{\sum_{i = 1}^n x_i^2}
        = |\alpha| \sqrt{\sum_{i = 1}^n x_i^2}
        = |\alpha| |x|_2
        , \\
    |\alpha x|_\infty
        &= \max_{1 \leq i \leq n} |\alpha x_i|
        = \max_{1 \leq i \leq n} |\alpha| |x_i|
        = |\alpha| \max_{1 \leq i \leq n} |x_i|
        = |\alpha| |x|_\infty
        .
\end{align*}
%
Hence all three functions satisfy the third (and final) property in the definition of a norm.

Thus we have shown that $|\cdot|_1$, $|\cdot|_2$, and $|\cdot|_\infty$ are norms.

\newpage

3. Show that if $g_m \in C^1([a, b], \R^n)$ for all $m \in \N$ and $\{g_m^\prime(t)\}$
is uniformly bounded on $[a, b]$, then $\{g_m(t)\}$ is equicontinuous on $[a, b]$.

\textit{Solution.}
Let $M > 0$ be such that for all $m \in \N$
%
\begin{equation*}
    |g_m^\prime(t)| < M
\end{equation*}
%
for all $t \in [a, b]$. Then using the Mean Value Theorem and applying
an absolute value to both sides, we have that
for all $m \in \N$, $t_1, t_2 \in [a, b]$
(without loss of generality taking $t_1 \leq t_2$),
%
\begin{equation*}
    |g_m(t_1) - g_m(t_2)| = |g_m^\prime(t^*)| |t_1 - t_2| < M |t_1 - t_2|
\end{equation*}
%
for some $t^* \in (t_1, t_2)$.

Now, fix any $\epsilon > 0$. Then if $t_1, t_2 \in [a, b]$ with
$|t_1 - t_2| < \epsilon / M$ we have that
%
\begin{equation*}
    |g_m(t_1) - g_m(t_2)| \leq M |t_1 - t_2| < \epsilon
\end{equation*}
%
for all $m \in \N$. Hence we have shown that $\{g_m(t)\}$ is equicontinuous.

\newpage

4. Determine if the following integral equation has a unique solution for some $\alpha > 0$:
%
\begin{equation*}
    x(t) = 2 + \int_0^t \frac{1}{3 + s^2} \sqrt{2 + x(s)^2} \dif s, \quad t \in [-\alpha, \alpha]
    .
\end{equation*}

\textit{Solution.}
Note that the above system is equivalent to the IVP
%
\begin{align*}
    x^\prime(t) &= f(t, x(t)), \quad t \in [-\alpha, \alpha], \\
    x(0) &= 2,
\end{align*}
%
where
%
\begin{equation*}
    f(t, x(t)) = \frac{1}{3 + t^2} \sqrt{2 + x(t)^2}.
\end{equation*}
%
Note that
%
\begin{align*}
    \dpd{f}{x} = \frac{x(t)}{(3 + t^2) \sqrt{2 + x(t)^2}}.
\end{align*}
%
is continuous on $\R^2$. Hence by Corollary 2.1 in the course notes,
we have that for some $\alpha > 0$
%
\begin{equation*}
    x(t) = 2 + \int_0^t \frac{1}{3 + s^2} \sqrt{2 + x(s)^2} \dif s, \quad t \in [-\alpha, \alpha]
    .
\end{equation*}
%
has a unique solution.

\newpage

5. Determine if the following IVP has a unique solution where $x(t) = (x_1(t), x_2(t))$:
%
\begin{align*}
    x_1^\prime(t) &= x_1(t) \sin(t) - 2 x_1(t)^2 - x_1(t) x_2(t), \\
    x_2^\prime(t) &= 2 t^2 x_2(t) - x_2(t)^2 - x_1(t) x_2(t), \\
    x(0) &= x_0.
\end{align*}

\textit{Solution.}
Let
%
\begin{equation*}
    f(t, x(t)) =
    \begin{pmatrix}
        x_1(t) \sin(t) - 2 x_1(t)^2 - x_1(t) x_2(t) \\
        2 t^2 x_2(t) - x_2(t)^2 - x_1(t) x_2(t)
    \end{pmatrix}
    .
\end{equation*}
%
Taking partial derivatives in $x$ we have
%
\begin{align*}
    \dpd{f_1}{x_1} &= \sin(t) - 4 x_1(t) - x_2(t), \\
    \dpd{f_2}{x_1} &= - x_2(t), \\
    \dpd{f_1}{x_2} &= - x_1(t), \\
    \dpd{f_2}{x_2} &= 2t^2 - 2 x_2(t) - x_1(t)
    .
\end{align*}
%
Clearly we have that $D_x f$ is continuous on $\R^3$ (since all
of its components are continuous), and
so by Corollary 2.1 in the course notes, we have that there exists
a unique solution $x(t)$ defined on $[-\alpha, \alpha]$ for some
$\alpha > 0$.

\newpage

6. Let $f \in C(Q, \R^n)$, where $Q \subset \R^{n + 1}$ is an open set. Assume
that $f(t, x)$ has continuous partial derivatives with respect to $x$. Show that
$f(t, x)$ is locally Lipschitz in $x$ in $Q$, and Lipschitz in $x$ in $x$ on any compact
and convex subset of $Q$.

\textit{Solution.}
First, let $(t_0, x_0) \in Q$ and let $Q_0 = Q_t \times Q_x \subset Q$,
where $Q_t \subset \R$ and $Q_x \subset \R^n$ such that $(t_0, x_0) \in Q_0$,
$t_0 \in Q_t$, $x_0 \in Q_x$, and also such that $Q, Q_t, Q_x$ are open, bounded,
and convex (they could be, for example, balls).
(Note that the Cartesian product of two open and convex sets is open and convex.)
Since $f(t, x)$ has continuous partial derivatives in $x$, it follows that
there exists a constant $M > 0$ such that $|D_x f(t, x)| < M$
for all $(t, x) \in \overline{Q_0}$ (and hence for all $(t, x) \in Q_0$).
This is because $\overline{Q_0}$ is closed and bounded, and hence compact;
so we can apply the Extreme Value Theorem.
Now, let $x, y \in Q_x$ and $t \in Q_t$. Define $g: [0, 1] \to \R^n$
by $g(s) = f(t, x + s(y - x))$. (Note that since $Q_x$ is convex, we have
that for all $s \in [0, 1]$, $x + s(y - x) \in Q_x$.) Thus, using the
Mean Value Theorem, we have that for some $c \in (0, 1)$,
%
\begin{align*}
    f(t, x) - f(t, y)
        &= g(1) - g(0) \\
        &= g^\prime(c) (1 - 0) \\
        &= g^\prime(c) \\
        &= (y - x) D_x f(t, x + c(y - x))
        ,
\end{align*}
%
where in the fourth line we used the chain rule
%
\begin{equation*}
    \dod{g}{s} = \dod{f(t, x + s(y - x))}{s} = D_x f(t, x + s(y - x)) \dod{(x + s(y - x))}{s} = D_x f(t, x+ s(y - x) \cdot (y - x)
    .
\end{equation*}
%
Taking a suitable norm to both sides of our main equation we have
%
\begin{equation*}
    |f(t, x) - f(t, y)|
        = \envert{(y - x) D_x f(t, x + c(y - x))}
        = |x - y| \envert{D_x f(t, x + c(y - x))}
        < |x - y| M
        .
\end{equation*}
%
Hence we have shown that for all $(t_0, x_0) \in \Q$, there exists an open subset $Q_0 \subset Q$
and a constant $M > 0$, where for all $(t, x), (t, y) \in Q_0$,
%
\begin{equation*}
    |f(t, x) - f(t, y)| < |x - y| M
    ,
\end{equation*}
%
so $f(t, x)$ is locally Lipschitz on $Q$.

Now suppose $Q^\prime \subset Q$ is any compact and convex subset of $Q$. Then
by the Extreme Value Theorem, we can find a constant $M > 0$ such that for all
$(t, x) \in \Q^\prime$, $|D_x f(t, x) < M|$. From here we can proceed exactly as
before and find that for all $(t, x), (t, y) \in Q^\prime$,
%
\begin{equation*}
    |f(t, x) - f(t, y)| < |x - y| M
    ,
\end{equation*}
%
and so $f(t, x)$ is Lipschitz on any compact and convex subset of $Q$.


\newpage

7. Let $f: \R^n \to \R^n$ be continuously differentiable. Show that two distinct solutions
of the autonomous system
%
\begin{equation*}
    x^\prime = f(x)
\end{equation*}
%
cannot intersect at a point, not even at different times.

\textit{Solution.}
Suppose $x, y$ are distinct solutions of the above autonomous system.
Since the solutions are distinct there exists $t_1 \in \R$ such that $x(t_1) \neq y(t_1)$.
Suppose for contradiction that there exists some point $t_2 \in \R$ such that $x(t_2) = y(t_2)$.
Now we need to consider two (exhaustive) cases.

The first case occurs when $t_1 < t_2$. Let $I \coloneqq [t_1, t_2]$, let
$J \coloneqq \{t \in I: x(t) = y(t)\}$, and let $t^* \coloneqq \inf(J)$.
Note that $t^*$ is well defined by the greatest-lower-bound property of the real numbers.
Because $x$ and $y$ are continuous (because they are differentiable) we have that
%
\begin{equation*}
    \lim_{t \to (t^*)^+} (x(t) - y(t)) = x(t^*) - y(t^*) = 0
    ,
\end{equation*}
%
since either $t^* \in J$ or there is a sequence $\{t_n\} \subseteq J$ such that $\{t_n\} \to t^*$.
Define $x^* \coloneqq x(t^*) = y(t^*)$. Using Picard's Existence and Uniqueness Theorem, we have
that there exists a unique solution to the IVP, for some $\alpha > 0$,
%
\begin{align*}
    x^\prime(t) &= f(x(t)), \quad t \in [t^* - \alpha, t^* + \alpha], \\
    x(t^*) &= x^*
\end{align*}
%
(note that we can apply the theorem because $f$ is continuously differentiable). Since both
$x$ and $y$ satisfy this IVP, we have that $x \equiv y$ on $[t^* - \alpha, t^* + \alpha]$.
But this contradicts that $x(t) \neq y(t)$ for $t < t^*$.

The second case occurs when $t_1 > t_2$ and the approach is nearly identical to the first case.
Let $I \coloneqq [t_2, t_1]$, let $J \coloneqq \{t \in I: x(t) = y(t)\}$, and let $t^* \coloneqq \sup(J)$.
Note that $t^*$ is well defined by the least-upper-bound property of the real numbers.
Similar to the first case we have that
%
\begin{equation*}
    \lim_{t \to (t^*)^-} (x(t) - y(t)) = x(t^*) - y(t^*) = 0
\end{equation*}
%
Define $x^* \coloneqq x(t^*) = y(t^*)$. Using Picard's Existence and Uniqueness Theorem, we have
that there exists a unique solution to the IVP, for some $\alpha > 0$,
%
\begin{align*}
    x^\prime(t) &= f(x(t)), \quad t \in [t^* - \alpha, t^* + \alpha], \\
    x(t^*) &= x^*
\end{align*}
%
Since both $x$ and $y$ satisfy this IVP, we have that $x \equiv y$ on $[t^* - \alpha, t^* + \alpha]$.
But this contradicts that $x(t) \neq y(t)$ for $t > t^*$.

Hence in either case we arrive at a contradiction, and thus we can conclude that there
is no such $t_2 \in \R$ such that $x(t_2) = y(t_2)$ for distinct solutions
$x$ and $y$ to the IVP $x^\prime(t) = f(x(t))$.

\end{document}
